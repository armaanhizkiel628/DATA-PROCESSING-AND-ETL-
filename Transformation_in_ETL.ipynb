{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 : Define Data Transformation in ETL and explain why it is important\n",
        "\n",
        "ANSWER 1\n",
        "\n",
        "1. Data transformation is the second step in the ETL process. It involves converting extracted data into a suitable format before loading it into a data warehouse.\n",
        "\n",
        "\n",
        "2. Why Data Transformation is Important\n",
        "\n",
        "* It cleans the data by removing errors, duplicates, and missing values.\n",
        "\n",
        "* It converts data formats (for example, date formats or data types).\n",
        "\n",
        "* It helps combine data from different sources into a single structure.\n",
        "\n",
        "* It improves data quality and consistency.\n",
        "\n",
        "* It makes data ready for analysis and reporting."
      ],
      "metadata": {
        "id": "cyYMAWtq4KdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 : List any four common activities involved in Data Cleaning.\n",
        "\n",
        "ANSWER 2\n",
        "\n",
        "1. Removing duplicate records\n",
        "\n",
        "2. Handling missing values\n",
        "\n",
        "3. Correcting incorrect or invalid data\n",
        "\n",
        "4. Standardizing data formats (such as dates and text)"
      ],
      "metadata": {
        "id": "stMWix-f486f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 : What is the difference between Normalization and Standardization?\n",
        "\n",
        "ANSWER 3\n",
        "\n",
        "1. Normalization\n",
        "\n",
        "* Meaning = \tOrganizing data to reduce redundancy\n",
        "* Purpose\tImproves data structure and storage\n",
        "* Used in = \tDatabases (design stage)\n",
        "* Focus = \tTable relationships and keys\n",
        "* Example\t= Splitting a table into smaller tables\n",
        "\n",
        "2. Standardization\n",
        "\n",
        "* Meaning = Making data follow a common format\n",
        "* Purpose = Improves data consistency\n",
        "* Used in = Data cleaning and transformation\n",
        "* Focus = Values, formats, and units\n",
        "* Example =  Converting all dates to one format"
      ],
      "metadata": {
        "id": "Xjy8PNmc5OyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4 : A dataset has missing values in the “Age” column. Suggest two techniques to handle this and explain when they should be used\n",
        "\n",
        "ANSWER 4\n",
        "\n",
        "*  Techniques to Handle Missing Values in the “Age” Column\n",
        "1. Replace with Average (Mean/Median) Age\n",
        "Missing age values can be filled using the average or median age of the dataset.\n",
        "When to use:\n",
        "\n",
        "* When only a few values are missing\n",
        "\n",
        "* When age values are fairly consistent and do not have extreme outliers\n",
        "\n",
        "2. Remove Records with Missing Age\n",
        "Rows containing missing age values can be deleted from the dataset.\n",
        "When to use:\n",
        "* When the number of missing values is very small\n",
        "* When age is not a critical attribute for analysis"
      ],
      "metadata": {
        "id": "ZExk5jGi6ynx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5 : Convert the following inconsistent “Gender” entries into a standardized format (“Male”, “Female”):[\"M\", \"male\", \"F\", \"Female\", \"MALE\", \"f\"]\n",
        "\n",
        "ANSWER\n",
        " 1. [\"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Female\"]\n",
        "\n",
        " 2. All variations are converted into a single consistent format to improve data quality and avoid confusion during analysis."
      ],
      "metadata": {
        "id": "mPoiNxQW7_LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6 : What is One-Hot Encoding? Give an example with the categories: “Red, Blue, Green”\n",
        "\n",
        "ANSWER\n",
        "\n",
        "1. One-Hot Encoding is a technique used to convert categorical data into numerical format so that it can be used in data processing and machine learning.\n",
        "\n",
        "2. EXAMPLE =\n",
        "| Color | Red | Blue | Green |\n",
        "| ----- | --- | ---- | ----- |\n",
        "| Red   | 1   | 0    | 0     |\n",
        "| Blue  | 0   | 1    | 0     |\n",
        "| Green | 0   | 0    | 1     |\n",
        "\n",
        "Each category is represented by a separate column. A value of 1 shows the presence of that category, and 0 shows absence."
      ],
      "metadata": {
        "id": "Fly8wdsk82ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7 : Explain the difference between Data Integration and Data Mapping in ETL.\n",
        "\n",
        "ANSWER\n",
        "\n",
        "1. DATA INTEGRATION\n",
        "* MEAN = Combining data from multiple sources\n",
        "* PURPOSE = Creates a unified dataset\n",
        "* FOCUS = Overall data combination\n",
        "* STAGE IN ETL = Happens during the ETL process\n",
        "* EXAMPLE = Merging data from CRM and sales systems\n",
        "\n",
        "2. DATA MAPING  \n",
        "* MEAN = Defining how source data matches target data\n",
        "* PURPOSE = Ensures correct data transfer\n",
        "* FOCUS = Field-to-field relationshiP\n",
        "* STAGE IN ETL = Done before or during transformation\n",
        "* EXAMPLE = Mapping cust_id to customer_id"
      ],
      "metadata": {
        "id": "bir4_GhY-Rgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8 : Explain why Z-score Standardization is preferred over Min-Max Scaling when outliers exist\n",
        "\n",
        "answer\n",
        "\n",
        "1. Min-Max Scaling scales data between 0 and 1 using the minimum and maximum values. When outliers are present, these extreme values can distort the entire scale, making most data points very close together.\n",
        "\n",
        "2. Z-score Standardization uses the mean and standard deviation to scale data. This method is less affected by extreme values, so the data remains more evenly distributed even when outliers exist.\n",
        "\n",
        "3. Z-score standardization is preferred because it handles outliers better and keeps the data distribution more balanced compared to Min-Max scaling.b"
      ],
      "metadata": {
        "id": "yzYxXKE4AHhf"
      }
    }
  ]
}